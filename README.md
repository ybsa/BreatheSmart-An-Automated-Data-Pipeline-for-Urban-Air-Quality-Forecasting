# ğŸŒ BreatheSmart: Production-Ready Air Quality Prediction System

![Python Version](https://img.shields.io/badge/python-3.8%2B-blue)
![License](https://img.shields.io/badge/license-MIT-green)
![Build Status](https://img.shields.io/badge/build-passing-brightgreen)

An end-to-end automated data pipeline for forecasting urban air quality in Abu Dhabi, with built-in bias detection and mitigation.

## ğŸ“‹ Overview

BreatheSmart is an automated system that:

- **Fetches** live air quality data from OpenAQ API (PM2.5, PM10, NO2, O3, SO2, CO)
- **Analyzes** geographic bias in sensor placement across neighborhoods
- **Predicts** PM2.5 levels for the next hour using machine learning
- **Delivers** health alerts via actionable forecasts

## ğŸ¯ The Problem & Solution

**Problem:** Urban residents lack hyper-local, predictive insights to plan outdoor activities safely. Sensor networks often have geographic bias, with fewer monitors in lower-income areas.

**Solution:** An automated pipeline that ingests, cleans, and analyzes air quality data to provide accurate forecasts with emphasis on sensor bias mitigation in under-represented neighborhoods.

---

## ğŸ—ï¸ Architecture

The system is built across **5 phases**:

1. **Automated Data Ingestor** - Daily data collection from OpenAQ
2. **EDA & Bias Analysis** - Geographic discrimination detection
3. **Feature Engineering** - Correction of data leakage, imputation, and lag generation
4. **ML Predictor** - XGBoost forecasting (RMSE ~2.9 Âµg/mÂ³)
5. **Operational Forecasting** - Automated prediction script

See [PROJECT_MAP.md](docs/PROJECT_MAP.md) for a detailed architecture diagram.

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- pip package manager

### Installation

1. **Clone the repository**

```bash
git clone <your-repo-url>
cd BreatheSmart-An-Automated-Data-Pipeline-for-Urban-Air-Quality-Forecasting
```

1. **Install dependencies**

```bash
pip install -r requirements.txt
```

1. **Get your OpenAQ API Key (FREE & REQUIRED)**
   - Visit [https://explore.openaq.org/register](https://explore.openaq.org/register)
   - Create a free account
   - Copy your API key from your dashboard

2. **Configure your environment**

```bash
# Option 1: Copy the example and edit
copy .env.example .env
# Then open .env and replace 'your_api_key_here' with your actual API key

# Option 2: Create .env directly
echo OPENAQ_API_KEY=your_actual_api_key_here > .env
```

---

## ğŸ› ï¸ Running the Pipeline

### Step 1: Data Collection

Fetch historical data (last 30 days) or start the daily ingestor:

```bash
python src/data_ingestor.py
```

*Output:* Raw CSV files saved to `data/raw/` (e.g., `data/raw/abudhabi_pm25_20251224_000905.csv`).

### Step 2: Feature Engineering

Process the raw data into a machine-learning-ready format:

```bash
python src/feature_engineering.py
```

*Output:* Processed dataset saved to `data/processed/training_data.csv`.

### Step 3: Model Training

Train the XGBoost Regressor on the processed data:

```bash
python src/model_training.py
```

*Output:*

- Trained model saved to `models/xgboost_pm25.json`
- Feature list saved to `models/model_features.pkl`

### Step 4: Generate Forecasts

Run the prediction engine to generate a forecast for the next hour:

```bash
python src/prediction.py
```

*Output:*

- Console output: `Forecast for 2024-01-01 10:00: 15.2 Âµg/mÂ³`
- Log file: `data/predictions.csv`

---

## ğŸ§ª Testing

We use `pytest` for unit and integration testing.

### Run all tests

```bash
pytest tests/
```

### Run with coverage report

```bash
pytest tests/ --cov=src --cov-report=html
```

---

## ğŸ¤– Automation

To run the ingestion pipeline on a schedule (e.g., daily at 2:00 AM):

```bash
python src/scheduler.py
```

To run a single test loop:

```bash
python src/scheduler.py --mode test
```

---

## ğŸ’» Web Dashboard

To launch the interactive dashboard:

```bash
streamlit run src/app.py
```

Features:

- **Real-time Metrics**: View current PM2.5 and forecasts.
- **Interactive Chart**: Explore historical trends.
- **Forecast Logs**: View prediction history.

---

## ğŸ“Š Project Structure

```
â”œâ”€â”€ src/                 # Source code
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ data_ingestor.py
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ model_training.py
â”‚   â”œâ”€â”€ prediction.py
â”‚   â””â”€â”€ scheduler.py
â”œâ”€â”€ tests/               # Test scripts
â”‚   â”œâ”€â”€ conftest.py      # Test fixtures
â”‚   â”œâ”€â”€ test_config.py
â”‚   â”œâ”€â”€ test_ingestor.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/             # Raw CSVs
â”‚   â”œâ”€â”€ processed/       # Cleaned training data
â”‚   â””â”€â”€ predictions.csv  # Forecast logs
â”œâ”€â”€ docs/                # Project Documentation
â”‚   â”œâ”€â”€ PROJECT_MAP.md
â”‚   â”œâ”€â”€ TEST_PLAN.md
â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”œâ”€â”€ .github/workflows/   # CI/CD pipelines
â”œâ”€â”€ logs/                # System logs
â”œâ”€â”€ models/              # Trained XGBoost artifacts
â”œâ”€â”€ notebooks/           # EDA and Analysis
â”œâ”€â”€ Dockerfile           # Container configuration
â”œâ”€â”€ docker-compose.yml   # Multi-service orchestration
â””â”€â”€ requirements.txt
```

---

## ğŸ³ Docker Deployment

### Quick Start with Docker

```bash
# Build and run the dashboard
docker compose up -d dashboard

# View logs
docker compose logs -f dashboard

# Stop
docker compose down
```

### Run Full Pipeline (with Scheduler)

```bash
# Run dashboard + automated scheduler
docker compose --profile full up -d
```

### Build Manually

```bash
# Build the image
docker build -t breathesmart .

# Run the dashboard
docker run -p 8501:8501 -e OPENAQ_API_KEY=your_key breathesmart
```

---

## ğŸ”„ CI/CD Pipeline

This project uses **GitHub Actions** for continuous integration:

- âœ… **Automated Testing** - Runs on Python 3.10, 3.11, 3.12
- âœ… **Code Quality** - Flake8 linting, Black formatting checks
- âœ… **Docker Build** - Validates container builds
- âœ… **Coverage Reports** - Uploaded to Codecov

### Workflow Status

Tests run automatically on every push and pull request to `main`/`master`.

---

## â˜ï¸ Cloud Deployment

### Streamlit Cloud (Recommended - Free)

1. Fork this repository
2. Go to [share.streamlit.io](https://share.streamlit.io/)
3. Connect your GitHub account
4. Deploy with:
   - **Main file path:** `src/app.py`
   - **Secrets:** Add `OPENAQ_API_KEY`

### Railway / Render

Use the included `Dockerfile` for one-click deployment.

---

## ğŸ¤– MLOps Features

### Experiment Tracking (MLflow)

```bash
# Start MLflow UI
mlflow ui
# View experiments at http://localhost:5000
```

### Model Monitoring

```bash
# Run health checks on predictions
python src/monitoring.py
```

### Data Versioning (DVC)

```bash
# Track training data
dvc add data/processed/training_data.csv
dvc push
```

### Pre-commit Hooks

```bash
# Install hooks
pip install pre-commit
pre-commit install

# Run manually
pre-commit run --all-files
```

See [MLOPS_GUIDE.md](docs/MLOPS_GUIDE.md) for detailed documentation.

---

## ğŸ“ License

Distributed under the MIT License. See `LICENSE` for more information.
